# -*- coding: utf-8 -*-
"""
Created on Tue Nov 19 17:14:02 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %%
import os
os.environ["NUMBA_NUM_THREADS"] = "100"


# %%
from pathlib import Path
import pandas as pd
import numpy as np
from functools import partial
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import yaml
import traceback


from utils.datautils import replace_column_suffixes, add_dataframe_to_dataframe_reindex, check_dataframe_consistency
from utils.dirutils import load_path_config, find_common_bid_ask_files
from utils.market import index_mapping, INDEX_SEQ, INDEX_ALL
from trans_operators import *
from utils.param import para_allocation
from utils.naming import generate_factor_names
from utils.timeutils import parse_time_string


# %%
def preprocess_daily_weights(daily_weights):
    daily_weights = daily_weights.drop(columns=[col for col in daily_weights.columns if col.endswith('.XBEI')])
    daily_weights = daily_weights.replace(0, np.nan).shift(1)
    return daily_weights


def preprocess_corr_weights_v0(weights):
    # 1. å‰å‘å¡«å……ç¼ºå¤±å€¼ï¼Œé™åˆ¶æœ€å¤šå¡«240ä¸ªè¿ç»­NaN
    weights = weights.fillna(method='ffill', limit=240)
    
    # 2. å°†å°äº0çš„æƒé‡è®¾ç½®ä¸º0
    weights = weights.where(weights >= 0, 0)
    
    # 3. å°†æ‰€æœ‰0çš„å€¼è®¾ä¸ºNaN
    weights = weights.replace(0, np.nan)
    
    return weights


def preprocess_corr_weights_v1(weights):
    # 1. å‰å‘å¡«å……ç¼ºå¤±å€¼ï¼Œé™åˆ¶æœ€å¤šå¡«240ä¸ªè¿ç»­NaN
    weights = weights.fillna(method='ffill', limit=240)
    
    # 2. å°†å°äº0çš„æƒé‡è®¾ç½®ä¸º0
    weights = weights.where(weights >= 0.1, 0)
    
    # 3. å°†æ‰€æœ‰0çš„å€¼è®¾ä¸ºNaN
    weights = weights.replace(0, np.nan)
    
    return weights


def preprocess_corr_weights_v2(weights):
    # 1. å‰å‘å¡«å……ç¼ºå¤±å€¼ï¼Œé™åˆ¶æœ€å¤šå¡«240ä¸ªè¿ç»­NaN
    weights = weights.fillna(method='ffill', limit=240)
    
    # 2. å°†å°äº0çš„æƒé‡è®¾ç½®ä¸º0
    weights = weights.where(weights >= 0.2, 0)
    
    # 3. å°†æ‰€æœ‰0çš„å€¼è®¾ä¸ºNaN
    weights = weights.replace(0, np.nan)
    
    return weights


def preprocess_corr_weights_ma_v0(weights):
    # 1. å‰å‘å¡«å……ç¼ºå¤±å€¼ï¼Œé™åˆ¶æœ€å¤šå¡«240ä¸ªè¿ç»­NaN
    weights = weights.fillna(method='ffill', limit=240)
    
    weights = weights.rolling(window=15).mean()
    
    # 2. å°†å°äº0çš„æƒé‡è®¾ç½®ä¸º0
    weights = weights.where(weights >= 0, 0)
    
    # 3. å°†æ‰€æœ‰0çš„å€¼è®¾ä¸ºNaN
    weights = weights.replace(0, np.nan)
    
    return weights


# %%
def process_ts_task(ts_name, factor, factor_name, target_dir, ts_func, save_first_layer=True):
    """å•ä¸ªä»»åŠ¡çš„å¤„ç†å‡½æ•°ï¼Œæ”¯æŒä¿å­˜ç¬¬ä¸€å±‚ç»“æœçš„é€‰é¡¹"""
    try:
        factor_ts = ts_func(factor)
        if save_first_layer:
            filename = factor_name if ts_name is None else f'{factor_name}-{ts_name}'
            factor_ts.to_parquet(target_dir / f'{filename}.parquet')
        return factor_ts
    except:
        traceback.print_exc()
        print(f"Error in processing {ts_name}")
        return None


def process_ts(factor, factor_name, ts_mapping, target_dir, ts_second_layer_mapping=None, save_first_layer=True):
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç† ts_mapping ä¸­çš„ä»»åŠ¡ï¼Œå¹¶æ”¯æŒç¬¬äºŒå±‚æ—¶åºå˜æ¢ã€‚
    
    Args:
        factor: è¾“å…¥å› å­æ•°æ®ã€‚
        factor_name: å› å­åç§°ã€‚
        ts_mapping: ç¬¬ä¸€å±‚æ—¶é—´åºåˆ—æ˜ å°„ï¼Œé”®ä¸ºåç§°ï¼Œå€¼ä¸ºå¤„ç†å‡½æ•°ã€‚
        target_dir: è¾“å‡ºç›®å½•ã€‚
        ts_second_layer_mapping: ç¬¬äºŒå±‚æ—¶é—´åºåˆ—æ˜ å°„ï¼Œé”®ä¸ºåç§°ï¼Œå€¼ä¸ºå­—å…¸ï¼ŒåŒ…å« 'base_ts' å’Œ 'func'ã€‚
        save_first_layer: æ˜¯å¦ä¿å­˜ç¬¬ä¸€å±‚è½¬æ¢ç»“æœã€‚
    """
    # åŠ¨æ€åˆ†é…è¿›ç¨‹æ•°
    max_workers = min(len(ts_mapping) if ts_mapping else 1, 30)
    first_layer_results = {}
    
    # å­˜å‚¨åŸå§‹å› å­ï¼Œä»¥ä¾¿å¯¹å…¶åº”ç”¨ç¬¬äºŒå±‚å˜æ¢
    first_layer_results['org'] = factor
    
    # å¤„ç†ç¬¬ä¸€å±‚å˜æ¢
    if ts_mapping:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            # æäº¤ç¬¬ä¸€å±‚ä»»åŠ¡
            for ts_name, ts_func in ts_mapping.items():
                task_func = partial(process_ts_task, ts_name, factor, factor_name, target_dir, ts_func, save_first_layer)
                futures[ts_name] = executor.submit(task_func)

            # è·å–ç¬¬ä¸€å±‚ä»»åŠ¡ç»“æœ
            for ts_name, future in tqdm(futures.items(), desc="Processing first layer"):
                try:
                    result = future.result()
                    if result is not None:
                        first_layer_results[ts_name] = result
                except Exception as e:
                    print(f"Error processing {ts_name}: {e}")
    
    # å¦‚æœæœ‰ç¬¬äºŒå±‚å˜æ¢ï¼Œåˆ™å¤„ç†
    if ts_second_layer_mapping:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            second_layer_futures = []
            
            for second_ts_name, config in ts_second_layer_mapping.items():
                base_ts_name = config['base_ts']
                second_ts_pr_name = config['second_ts_pr_name']
                second_ts_func = config['func']
                
                # ç¡®ä¿åŸºç¡€æ—¶åºå­˜åœ¨
                if base_ts_name in first_layer_results:
                    base_factor = first_layer_results[base_ts_name]
                    # å¯¹äºåŸå§‹å› å­ï¼Œä½¿ç”¨åŸå§‹å› å­åç§°ï¼›å¯¹äºç¬¬ä¸€å±‚å˜æ¢ï¼Œä½¿ç”¨å¤åˆåç§°
                    if base_ts_name == 'org':
                        first_factor_name = factor_name
                    else:
                        first_factor_name = f"{factor_name}-{base_ts_name}"
                    
                    # åˆ›å»ºç¬¬äºŒå±‚ä»»åŠ¡
                    task = partial(process_ts_task, second_ts_pr_name, base_factor, first_factor_name, 
                                  target_dir, second_ts_func, True)  # ç¬¬äºŒå±‚ç»“æœæ€»æ˜¯ä¿å­˜
                    second_layer_futures.append(executor.submit(task))
            
            # ç­‰å¾…æ‰€æœ‰ç¬¬äºŒå±‚ä»»åŠ¡å®Œæˆ
            for future in tqdm(as_completed(second_layer_futures), total=len(second_layer_futures), desc="Processing second layer"):
                try:
                    future.result()
                except Exception as e:
                    print(f"Error in second layer processing: {e}")


class GenerateFactors:
    
    target_folder = {
        'init': 'cs',
        'update': 'incremental_cs',
        }
    
    def __init__(self, generate_name, ind_cate, ind_list=None, ind_dir=None, n_workers=1, mode='init'):
        self.generate_name = generate_name
        self.ind_cate = ind_cate
        self.ind_list = ind_list
        self.ind_dir = Path(ind_dir) if ind_dir is not None else None
        self.n_workers = n_workers
        self.mode = mode
        
        self._load_path_config()
        self._load_param()
        self._init_dir()
        self._load_daily_weights()
        self._load_selfdefined_weights()
        self._get_aggregation_mapping()
        self._get_ts_mapping()
        self._get_generate_one_func()
        self._get_ind_list()
        
    def _load_path_config(self):
        project_dir = Path(__file__).resolve().parents[1]
        path_config = load_path_config(project_dir)
        
        self.lob_ind_dir = Path(path_config['lob_indicators'])
        self.factor_dir = Path(path_config['factors'])
        self.daily_weights_dir = Path(path_config['daily_weights'])
        self.param_dir = Path(path_config['param'])
        
    def _init_dir(self):
        self.ind_cate_dir = self.ind_dir or self.lob_ind_dir / self.ind_cate / self.target_folder[self.mode]
        self.target_dir = self.factor_dir / self.ind_cate / self.generate_name
        self.target_dir.mkdir(exist_ok=True, parents=True)
        # åˆ›å»ºä¸“é—¨çš„orgæ•°æ®ç›®å½•
        org_name = self.params.get('org_name', 'org')
        self.org_dir = self.factor_dir / self.ind_cate / org_name
        self.org_dir.mkdir(exist_ok=True, parents=True)
        self.debug_dir = self.factor_dir / 'debug' / self.ind_cate / self.generate_name
        self.debug_dir.mkdir(exist_ok=True, parents=True)
        
    def _load_param(self):
        with open(self.param_dir / 'from_cs' / f'{self.generate_name}.yaml', "r") as file:
            self.params = yaml.safe_load(file)
        
        # è·å–æ•°æ®åŠ è½½ä½ç½®ç›¸å…³é…ç½®
        self.load_from_org = self.params.get('load_from_org', True)
        self.load_from_target = self.params.get('load_from_target', True)
        
        # è·å–æ•°æ®ä¿å­˜ä½ç½®ç›¸å…³é…ç½®
        self.save_to_org = self.params.get('save_to_org', True)
        self.save_org_to_target = self.params.get('save_org_to_target', True)
        
    def _load_daily_weights(self):
        index_list = self.params['index_weights_to_load']
        
        daily_weights = {}
        for index_name in index_list:
            index_code = index_mapping.get(index_name) or index_name
            daily_weight = pd.read_parquet(self.daily_weights_dir / f'{index_code}.parquet')
            daily_weights[index_code] = preprocess_daily_weights(daily_weight)
        self.daily_weights = daily_weights
        
    def _load_selfdefined_weights(self):
        selfdefined_weights_params = self.params.get('selfdefined_weights_params')
        weight_dir = Path(selfdefined_weights_params['weight_dir'])
        index_list = selfdefined_weights_params['index_list']
        suffix = selfdefined_weights_params['suffix']
        process_weight_func = globals()[selfdefined_weights_params['process_weight_func']]
        weights = {}
        for index_name in index_list:
            index_code = index_mapping.get(index_name) or index_name
            weight = pd.read_parquet(weight_dir / f'{index_code}_{suffix}.parquet')
            weights[index_code] = process_weight_func(weight)
        self.selfdefined_weights = weights
        
    def _get_aggregation_mapping(self):
        aggregation_info = self.params['aggregation']
        target_indexes = [index_mapping.get(index_name, index_name) for index_name in self.params.get('target_indexes', INDEX_SEQ)]
        index_all = index_mapping[self.params.get('index_all', INDEX_ALL)]
        index_seq = [index_mapping[index_name] for index_name in self.params.get('index_seq', [])]
        if len(index_seq) == 0:
            index_seq = target_indexes
        
        aggregation_mapping = {}
        for agg_name, agg_info in aggregation_info.items():
            combined_name = agg_info['aggregation']
            agg_func = globals()[combined_name]
            downscale_depth = agg_info.get('downscale_depth', None)
            imb_name = agg_info.get('imb')
            imb_func = globals()[imb_name] if imb_name else None
            ts_name = agg_info.get('ts')
            ts_func = globals()[ts_name] if ts_name else None
            ts_prs = agg_info.get('ts_prs')
            if ts_prs is not None:
                for ts_pr in ts_prs:
                    if 'window' in ts_pr and isinstance(ts_prs[ts_pr], str):
                        ts_prs[ts_pr] = int(parse_time_string(ts_prs[ts_pr]) / 60) # !!!: å¾…ä¼˜åŒ–æˆç”±æ•°æ®é¢‘ç‡å†³å®š
            exchange = agg_info.get('exchange', 'all')
            ts_func_with_pr = partial(ts_func, **ts_prs) if ts_func is not None else None
            cs_name = agg_info.get('cs', 'csmean')
            cs_func = globals()[cs_name] if cs_name else None
            # å°†å‚æ•°æ•´ç†æˆå­—å…¸
            params = {
                "daily_weights": self.daily_weights,
                "target_indexes": target_indexes,
                "index_all": index_all,
                "index_seq": index_seq,
                "downscale_depth": downscale_depth,
                "imb_func": imb_func,
                "ts_func_with_pr": ts_func_with_pr,
                "cs_func": cs_func,
                "n_workers": self.n_workers,
                "exchange": exchange
            }
            
            # æ£€æŸ¥selfæ˜¯å¦æœ‰selfdefined_weightså±æ€§ï¼Œå¦‚æœæœ‰åˆ™æ·»åŠ åˆ°paramså­—å…¸ä¸­
            if hasattr(self, 'selfdefined_weights'):
                params["selfdefined_weights"] = self.selfdefined_weights
            
            # ä½¿ç”¨ ** è§£åŒ…å­—å…¸ï¼Œå°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™ partial
            aggregation_mapping[agg_name] = partial(agg_func, **params)
        self.aggregation_mapping = aggregation_mapping
        
    def _get_ts_mapping(self):
        ts_info = self.params['ts']
        
        # å¤„ç†ç¬¬ä¸€å±‚æ—¶åºå˜æ¢
        ts_mapping = {}
        ts_name_groups = {}  # å­˜å‚¨æŒ‰åŸºç¡€è½¬æ¢å‡½æ•°ååˆ†ç»„çš„ts_pr_names
        
        for ts_name, ts_prs in ts_info.items():
            ts_pr_names = generate_factor_names(ts_name, ts_prs)
            ts_pr_list = para_allocation(ts_prs)
            ts_func = globals()[ts_name]
            
            # æŒ‰åŸºç¡€å‡½æ•°åï¼ˆå¦‚'stdz', 'ma'ç­‰ï¼‰åˆ†ç»„å­˜å‚¨æ‰€æœ‰å‚æ•°ç»„åˆ
            ts_name_groups[ts_name] = []
            
            for ts_pr_name, ts_pr in zip(ts_pr_names, ts_pr_list):
                full_name = f"{ts_name}_{ts_pr_name}" if not ts_pr_name.startswith(f"{ts_name}_") else ts_pr_name
                ts_mapping[full_name] = partial(ts_func, **ts_pr)
                ts_name_groups[ts_name].append(full_name)
        
        self.ts_mapping = ts_mapping
        self.ts_name_groups = ts_name_groups
        
        # å¤„ç†ç¬¬äºŒå±‚æ—¶åºå˜æ¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.ts_second_layer_mapping = {}
        ts_second_layer_info = self.params.get('ts_second_layer', {})
        
        for second_ts_name, second_ts_config in ts_second_layer_info.items():
            base_ts_names = second_ts_config.get('base_ts', [])
            second_ts_prs = second_ts_config.get('params', {})
            
            # ç›´æ¥ä½¿ç”¨ second_ts_name ä½œä¸ºå‡½æ•°å
            second_ts_func = globals()[second_ts_name]
            
            # ç”Ÿæˆå‚æ•°åå’Œå‚æ•°åˆ—è¡¨
            second_ts_pr_names = generate_factor_names(second_ts_name, second_ts_prs)
            second_ts_pr_list = para_allocation(second_ts_prs)
            
            # ç¡®å®šåŸºç¡€æ—¶åºå˜æ¢åˆ—è¡¨
            effective_base_ts = []
            
            # ç¡®ä¿ 'org' æ€»æ˜¯åŒ…å«åœ¨åŸºç¡€å˜æ¢åˆ—è¡¨ä¸­ä»¥æ”¯æŒå¯¹åŸå§‹å› å­åº”ç”¨ç¬¬äºŒå±‚å˜æ¢
            effective_base_ts.append('org')
            
            # å¦‚æœbase_tsä¸ºç©ºï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰ç¬¬ä¸€å±‚å˜æ¢
            if not base_ts_names:
                effective_base_ts.extend(list(ts_mapping.keys()))
            else:
                # éå†æ¯ä¸ªbase_tsï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åŸºç¡€å‡½æ•°å
                for base_name in base_ts_names:
                    if base_name in ts_name_groups:
                        # å¦‚æœæ˜¯åŸºç¡€å‡½æ•°åï¼Œæ·»åŠ è¯¥å‡½æ•°çš„æ‰€æœ‰å‚æ•°ç»„åˆ
                        effective_base_ts.extend(ts_name_groups[base_name])
                    elif base_name in ts_mapping:
                        # å¦‚æœæ˜¯å®Œæ•´å‡½æ•°åï¼ˆå¸¦å‚æ•°çš„ï¼‰ï¼Œç›´æ¥æ·»åŠ 
                        effective_base_ts.append(base_name)
            
            # ä¸ºæ¯ä¸ªæœ‰æ•ˆçš„åŸºç¡€å˜æ¢ç”Ÿæˆç¬¬äºŒå±‚å˜æ¢
            for base_ts_name in effective_base_ts:
                for second_ts_pr_name, second_ts_pr in zip(second_ts_pr_names, second_ts_pr_list):
                    # æ„å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼ŒåŒ…å«åŸºç¡€å˜æ¢å’Œç¬¬äºŒå±‚å˜æ¢çš„ä¿¡æ¯
                    full_name = f'{base_ts_name}-{second_ts_name}_{second_ts_pr_name}'
                    self.ts_second_layer_mapping[full_name] = {
                        'base_ts': base_ts_name,
                        'second_ts_pr_name': second_ts_pr_name,
                        'func': partial(second_ts_func, **second_ts_pr)
                    }
        
        # è·å–æ˜¯å¦ä¿å­˜ç¬¬ä¸€å±‚ç»“æœçš„é…ç½®
        self.save_first_layer = self.params.get('save_first_layer', True)
        
    def _get_generate_one_func(self):
        self.generate_one = partial(generate_one, ind_dir=self.ind_cate_dir, 
                                    target_dir=self.target_dir,
                                    org_dir=self.org_dir,
                                    aggregation_mapping=self.aggregation_mapping,
                                    ts_mapping=self.ts_mapping,
                                    ts_second_layer_mapping=self.ts_second_layer_mapping,
                                    save_first_layer=self.save_first_layer,
                                    mode=self.mode,
                                    load_from_org=self.load_from_org, load_from_target=self.load_from_target,
                                    save_to_org=self.save_to_org, save_org_to_target=self.save_org_to_target,
                                    debug_dir=self.debug_dir)
        
    def _get_ind_list(self):
        self.ind_list = self.ind_list or find_common_bid_ask_files(self.ind_cate_dir)
        
    def run(self):
        if self.n_workers is None or self.n_workers == 1:
            for ind_name in self.ind_list:
                self.generate_one(ind_name)
        else:
            with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
                all_tasks = [executor.submit(self.generate_one, ind_name)
                             for ind_name in self.ind_list]
                num_of_success = 0
                for task in tqdm(as_completed(all_tasks), total=len(all_tasks), desc=f'{self.ind_cate}'):
                    res = task.result()
                    if res == 0:
                        num_of_success += 1
                print(f'num_of_success: {num_of_success}, num_of_failed: {len(self.ind_list)-num_of_success}')


def generate_one(ind_name, ind_dir, target_dir, org_dir, aggregation_mapping, ts_mapping, 
                mode='init', ts_second_layer_mapping=None, save_first_layer=True, 
                load_from_org=True, load_from_target=True,
                save_to_org=True, save_org_to_target=True,
                debug_dir=Path('./debug')):
    """
    ç”ŸæˆæŒ‡æ ‡åŠå…¶æ—¶åºå˜æ¢ï¼Œæ”¯æŒç²¾ç¡®æ§åˆ¶æ•°æ®åŠ è½½å’Œä¿å­˜ä½ç½®
    
    å‚æ•°:
    - ind_name: æŒ‡æ ‡åç§°
    - ind_dir: æŒ‡æ ‡æ•°æ®ç›®å½•
    - target_dir: ç›®æ ‡è¾“å‡ºç›®å½•
    - org_dir: åŸå§‹å› å­å­˜å‚¨ç›®å½•
    - aggregation_mapping: èšåˆå‡½æ•°æ˜ å°„
    - ts_mapping: æ—¶åºå˜æ¢æ˜ å°„
    - mode: æ¨¡å¼ï¼Œ'init'æˆ–'update'
    - ts_second_layer_mapping: äºŒçº§æ—¶åºå˜æ¢æ˜ å°„
    - save_first_layer: æ˜¯å¦ä¿å­˜ä¸€çº§æ—¶åºå˜æ¢ç»“æœ
    - load_from_org: æ˜¯å¦ä»org_diråŠ è½½æ•°æ®
    - load_from_target: æ˜¯å¦ä»target_diråŠ è½½æ•°æ®
    - save_to_org: æ˜¯å¦ä¿å­˜æ•°æ®åˆ°org_dir
    - save_to_target: æ˜¯å¦ä¿å­˜æ•°æ®åˆ°target_dir
    - debug_dir: è°ƒè¯•ä¿¡æ¯ç›®å½•
    """
    try:
        ind_sides = {side: pd.read_parquet(ind_dir / f'{side}_{ind_name}.parquet') # replace_column_suffixes
                     for side in ('Bid', 'Ask')}
    except:
        ind_sides = {'Bid': pd.DataFrame(), 'Ask': pd.DataFrame()}
    
    for agg_name, agg_func in aggregation_mapping.items():
        # æ„å»ºå¯èƒ½çš„å› å­åç§°
        possible_factor_names = {
            'single': {'single': f'{ind_name}-{agg_name}'}, 
            'combo': {'Bid': f'{ind_name}-{agg_name}_Bid', 'Ask': f'{ind_name}-{agg_name}_Ask'}
        }
        
        # å°è¯•åŠ è½½ç°æœ‰æ•°æ®
        res = None
        search_locations = []
        
        # æ ¹æ®åŠ è½½é€‰é¡¹ç¡®å®šæœç´¢ä½ç½®
        if load_from_target:
            search_locations.append(('target', target_dir))
        if load_from_org:
            search_locations.append(('org', org_dir))
            
        # æŒ‰ä¼˜å…ˆçº§æœç´¢æ•°æ®
        for location_name, location_dir in search_locations:
            for output_type, factor_names in possible_factor_names.items():
                path_dict = {tag: location_dir / f'{factor_name}-org.parquet'
                             for tag, factor_name in factor_names.items()}
                
                if all([os.path.exists(path) for path in path_dict.values()]):
                    if output_type == 'single':
                        res = pd.read_parquet(path_dict['single'])
                        print(f"Loading factor from {path_dict['single']} ({location_name})")
                        break
                    elif output_type == 'combo':
                        res = {tag: pd.read_parquet(factor_path) for tag, factor_path in path_dict.items()}
                        print(f"Loading factors from {', '.join(str(p) for p in path_dict.values())} ({location_name})")
                        break
            if res is not None:
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç°æœ‰æ•°æ®ï¼Œåˆ™è®¡ç®—æ–°æ•°æ®
        if res is None:
            print(f"Computing factor for {ind_name}-{agg_name}")
            res = agg_func(ind_sides)
            
        # å¤„ç†æ›´æ–°æ¨¡å¼
        if mode == 'update':
            res_inc = agg_func(ind_sides)
            
            if isinstance(res, pd.DataFrame):
                # å•å› å­æƒ…å†µ
                factor_name = f'{ind_name}-{agg_name}'
                status, info = check_dataframe_consistency(res, to_float32(res_inc))
                
                if status == "INCONSISTENT":
                    # å°†ä¸ä¸€è‡´çš„æ–°æ•°æ®ä¿å­˜åˆ°debugç›®å½•
                    debug_path = debug_dir / f'{factor_name}-inconsistent.parquet'
                    to_float32(res_inc).to_parquet(debug_path)
                    
                    # æŠ¥é”™å¹¶é€€å‡º
                    error_msg = (f"Data inconsistency detected for factor {factor_name}: "
                                f"At index {info['index']}, column {info['column']}, "
                                f"original value: {info['original_value']}, new value: {info['new_value']}. "
                                f"Total {info['inconsistent_count']} inconsistencies found. "
                                f"Debug data saved to {debug_path}")
                    raise ValueError(error_msg)
            
            elif isinstance(res, dict):
                # ç»„åˆå› å­æƒ…å†µ
                for tag in list(res.keys()):
                    factor_name = f'{ind_name}-{agg_name}_{tag}'
                    status, info = check_dataframe_consistency(res[tag], to_float32(res_inc[tag]))
                    
                    if status == "INCONSISTENT":
                        # å°†ä¸ä¸€è‡´çš„æ–°æ•°æ®ä¿å­˜åˆ°debugç›®å½•
                        debug_path = debug_dir / f'{factor_name}-inconsistent.parquet'
                        to_float32(res_inc[tag]).to_parquet(debug_path)
                        
                        # æŠ¥é”™å¹¶é€€å‡º
                        error_msg = (f"Data inconsistency detected for factor {factor_name}: "
                                    f"At index {info['index']}, column {info['column']}, "
                                    f"original value: {info['original_value']}, new value: {info['new_value']}. "
                                    f"Total {info['inconsistent_count']} inconsistencies found. "
                                    f"Debug data saved to {debug_path}")
                        raise ValueError(error_msg)
            
            # å¦‚æœä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡æˆ–æ²¡æœ‰è¿›è¡Œæ£€æŸ¥ï¼Œåˆ™ç»§ç»­æ›´æ–°
            if isinstance(res, pd.DataFrame):
                res = add_dataframe_to_dataframe_reindex(res, to_float32(res_inc))
            elif isinstance(res, dict):
                res = {tag: add_dataframe_to_dataframe_reindex(res[tag], to_float32(res_inc[tag])) for tag in list(res.keys())}
        
        # å¤„ç†å•å› å­æˆ–ç»„åˆå› å­
        if isinstance(res, pd.DataFrame):
            factor_name = f'{ind_name}-{agg_name}'
            factor = to_float32(res).dropna(how='all')
            
            # æ ¹æ®ä¿å­˜é€‰é¡¹ä¿å­˜orgå› å­
            save_locations = []
            if save_to_org:
                save_locations.append(('org', org_dir))
            if save_org_to_target:
                save_locations.append(('target', target_dir))
                
            for location_name, location_dir in save_locations:
                factor.to_parquet(location_dir / f'{factor_name}-org.parquet')
                print(f"Saved factor to {location_dir / f'{factor_name}-org.parquet'} ({location_name})")
                
            # å¤„ç†æ—¶åºå˜æ¢
            process_ts(factor, factor_name, ts_mapping, target_dir, ts_second_layer_mapping, save_first_layer)
        elif isinstance(res, dict):
            for res_name, factor in res.items():
                factor_name = f'{ind_name}-{agg_name}_{res_name}'
                factor = to_float32(factor).dropna(how='all')
                
                # æ ¹æ®ä¿å­˜é€‰é¡¹ä¿å­˜orgå› å­
                save_locations = []
                if save_to_org:
                    save_locations.append(('org', org_dir))
                if save_org_to_target:
                    save_locations.append(('target', target_dir))
                    
                for location_name, location_dir in save_locations:
                    factor.to_parquet(location_dir / f'{factor_name}-org.parquet')
                    print(f"Saved factor to {location_dir / f'{factor_name}-org.parquet'} ({location_name})")
                
                # å¤„ç†æ—¶åºå˜æ¢
                process_ts(factor, factor_name, ts_mapping, target_dir, ts_second_layer_mapping, save_first_layer)
    return 0